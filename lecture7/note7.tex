%\usepackage{xspace}

\newcommand{\bits}{\{0,1\}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfv}{\mathbf{v}}
\newcommand{\bfp}{\mathbf{p}}
\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfy}{\mathbf{y}}

\newcommand{\calR}{\mathcal{R}}

\newcommand{\CNextMsg}{\ensuremath{{\sf C Next Msg}}}
\newcommand{\SNextMsg}{\ensuremath{{\sf S Next Msg}}}
\newcommand{\CNext}{\ensuremath{{\sf C Next}}}
\newcommand{\SNext}{\ensuremath{{\sf S Next}}}
\newcommand{\Cstp}{\ensuremath{{\sf Cst'}}}
\newcommand{\Cst}{\ensuremath{{\sf Cst}}}
\newcommand{\msg}{\ensuremath{{\sf msg}}}
\newcommand{\msgp}{\ensuremath{{\sf msg'}}}
\newcommand{\Coutput}{\ensuremath{{\sf Reconstr}}}
\newcommand{\ans}{\ensuremath{{\sf ans}}}
\newcommand{\Ccoins}{\ensuremath{{\sf Ccoins}}}
\newcommand{\Scoins}{\ensuremath{{\sf Scoins}}}
%\newcommand{\Comm}{\ensuremath{{\sf Comm}}}
\newcommand{\Expt}{\ensuremath{{\sf Expt}}}
\newcommand{\coin}{\ensuremath{{\sf coin}}}
\newcommand{\View}{\ensuremath{{\sf View}}}
%\newcommand{\negl}{\ensuremath{{\sf negl}}}
\newcommand{\PPT}{PPT }
\newcommand{\Out}{\ensuremath{{\sf Out}}}
\newcommand{\OWF}{\ensuremath{{\sf OWF}}}
\newcommand{\OT}{\ensuremath{{\sf OT}}}
\newcommand{\PIR}{\ensuremath{{\sf PIR}}}
\newcommand{\Server}{\ensuremath{{\sf Server}}}
\newcommand{\Client}{\ensuremath{{\sf Client}}}
\newcommand{\Alice}{\ensuremath{{\sf Alice}}}
\newcommand{\Bob}{\ensuremath{{\sf Bob}}}
%\newcommand{\getr}{\ensuremath{~{\overset{\$}{\leftarrow}}}~}
\newcommand{\get}{\ensuremath{\leftarrow}}
\newcommand{\E}{\ensuremath{{\bf E}}}
\newcommand{\out}{\ensuremath{{\sf out}}}
%\newcommand{\PRF}{\ensuremath{{\sf PRF}}}
\newcommand{\prob}{\ensuremath{\mathbb{P}}}
\newcommand{\known}{\ensuremath{{\sf known}}}
\newcommand{\enc}{\ensuremath{{\sf enc}}}
\newcommand{\db}{\ensuremath{{\sf DB}}}
\newcommand{\bbad}{\ensuremath{{\sf b_{bad}}}}
\newcommand{\bgood}{\ensuremath{{\sf b_{good}}}}

%\setlength{\parindent}{0pt}

%This is our final lecture on private information retrieval. 
In this lecture, we will prove a lower bound about the client space and server computation 
tradeoff for preprocessing PIR schemes. 
%For the proof, we will use techniques from complexity theory literature.
We will borrow techniques for proving time-space tradeoff
from the complexity theory literature.

Specifically, we consider a 1-server preprocessing PIR scheme 
with the following syntax:
\begin{itemize}[leftmargin=7mm]
\item {\it Preprocessing algorithm.}
Suppose there is a (possibly randomized and unbounded) %, possibly randomized
preprocessing function denoted 
${\sf Prep} : \{0, 1\}^n \rightarrow \{0, 1\}^S$ 
that takes in an $n$-bit database $\DB\in\{0, 1\}^n$ as input,
and outputs an $S$-bit hint string denoted $h$.
\item 
{\it A single query.} 
The client and the server perform a (possibly randomized) query protocol.  
The client takes in $h$ and some index $i \in [n]$ as input, and the 
server takes 
the database $\DB\in \{0, 1\}^n$ as input. 
%Then, the client engages in a query protocol with the server,
%to learn the database at some index  
%$i \in [n]$.
To answer the query, the server is allowed to read at most $T$ locations of the database. 
\end{itemize}
In other words, we do not place any restriction
on the amount of work performed
during preprocessing. The only constraints
are that at the end of the preprocessing: 
1) the client  
is allowed to store only the hint $h$; and 2) the server
is allowed to store only the original database $\DB$ and no extra information.

%Both the preprocessing function ${\sf Prep}$
%and the query protocol can be randomized. 
We assume perfect correctness, i.e., for any $\DB \in \{0, 1\}^n$, any query $i \in [n]$, 
correctness holds with probability $1$.
Let ${\sf view}_S(\DB, i)$ 
denote the server's view during the query phase 
when we run the PIR scheme (i.e., preprocessing followed by the query protocol)
over inputs $\DB$ and $i$.
For security, we require 
that for any $\DB \in \{0, 1\}^n$, any $i, j \in [n]$,  
${\sf view}_S(\DB, i) \approx {\sf view}_S(\DB, j)$
where $\approx$ means computational indistinguishability.

For such a preprocessing PIR scheme, we will prove a tradeoff
between $S$ and $T$ as stated in the following theorem: 


\begin{theorem}[Time space tradeoff for preprocessing PIR~\cite{CK20}]
    Given a 1-server preprocessing PIR, 
        let $S$ be the client space, and let $T$ be the server compuation per query. 
    Then,  
    $(S+1)(T+1) \ge N$. \elaine{TODO: edit the theorem based on what we can prove later}
\label{thm:lb}
\end{theorem}

Again, note that the lower bound 
holds even when we allow the preprocessing to be unbounded, 
even when there is only a single query
after the preprocessing, 
and even when 
the query phase may have arbitrarily many rounds of interaction. 



\paragraph{Piano.}
Recall that in an earlier lecture, 
we covered Piano~\cite{piano}, a preprocessing 1-server PIR scheme. 
%It uses the client-specific preprocessing model, meaning that each client has a subscription phase with the server, during which it will perform preprocessing. 
Piano enjoys 
the following performance bounds:
\begin{itemize}[leftmargin=7mm]
    \item Client space: $\widetilde{O}(\sqrt{n})$
where $\widetilde{O}(\cdot)$ hides a(n  arbitrarily small) superlogarithmic function.
    \item Communication per query: $O(\sqrt{n})$
    \item Server computation per query: ${O}(\sqrt{n})$
\end{itemize}

We can see that Piano achieves optimal 
client space and server computation tradeoff (up to polylogarithmic factors)
in light of \Cref{thm:lb}.
%Consider the server computation and client space trade-off. For PIANO, this trade-off [Client Space: $\widetilde{O}(\sqrt{n})$, Compute per Query: $\widetilde{O}(\sqrt{n})$] is nearly optimal up to poly-log factors. 


%This is because of the following lower bound.
%\paragraph{Space/Computation Lower Bound}


%This lower bound holds even for a scheme that supports 1 query, regardless of server space, bandwidth, and amount of preprocessing work. 

\section{Warmup: Yao's Box Problem}
Before proving \Cref{thm:lb}, 
we first prove a time-space tradeoff for a classical problem
called the Yao's box problem~\cite{yao}, and we shall
see why 
Yao's box problem is closely related to preprocessing PIR.

We have a server with $n$ boxes, each covering a bit. 
Henceforth we use $\DB := (\DB[1], \ldots, \DB[n])$ to denote the $n$ bits.
Consider the following game:
\begin{itemize}[leftmargin=5mm]
\item 
{\it Preprocessing.}
During a preprocessing phase, the server and client can 
perform an unbounded amount of computation. At the end of
preprocessing, the client obtains an $S$-bit hint $h$;
the server does not store any extra information
besides $\DB$ itself where each bit is covered by some box. 
\item 
{\it Query.}
The client wants to find out  
$\DB[i]$ for some index $i \in [n]$.
The client and server 
now engage in some protocol at the end of which the client outputs
an answer $\beta$.
The query protocol must satisfy two requirements:
1) the server is {\it not allowed to open box $i$}
during the protocol; and 2) the server 
can open at most $T$ boxes.
\end{itemize}

We allow the preprocessing algorithm and query 
protocol to be possibly randomized.
We require that the protocol to have {\it perfect correctness},
i.e., for any $\DB \in \{0, 1\}^n$, 
any index $i \in [n]$, 
after performing the preprocessing and 
the query protocol for index $i$, the client's output $\beta = \DB[i]$ with 
probability $1$. 


Note that unlike PIR, 
Yao's box problem does not have any privacy requirement.
In particular, it is perfectly ok for the query 
index $i$ to be leaked
to the server.



\ignore{
\paragraph{Preprocessing Phase}
Client and server can open all bits, and run arbitrary and unbounded computations. However, we have a constraint that at the end of the preprocessing, client can only store $S$ bits of information.

\paragraph{Query}
Client wants to know the i-th bit. We allow the client and server to have unbounded communication and work. We have a constraint that the server can open at most $T$ boxes and it cannot open box $i$.
}

In the above formulation, $S$ denotes the client space
at the end of preprocessing, and $T$ can be viewed as a lower bound on the 
server's running time during the query phase. 
We care about characterizing the tradeoff between $S$ and $T$,
that is, the client-space and server-time tradeoff.

Yao~\cite{yao} proved the following theorem:

\begin{theorem}[Yao's box problem]
$S \cdot (T+1) \geq n$. \elaine{TODO: modify the statement
based on proof}  
\label{thm:yaobox}
\end{theorem} 


\subsection{Upper Bound}
Before proving \Cref{thm:yaobox}, 
let us first see a simple upper bound that can match 
the $S\cdot (T+1) = n$ tradeoff.
For simplicity, assume $n$ is a perfect square.
\begin{enumerate}
    \item Divide the $n$ boxes into $\sqrt{n}$ segments each
of size $\sqrt{n}$. 
    \item Preprocessing: 
the client stores the parity of each segment, denoted $p_1, \ldots, p_{\sqrt{n}}$
respectively.
    \item Query for index $i \in [n]$: 
suppose $i$ belongs 
to the $j$-th segment. 
The server opens 
every other box 
except $i$ in the $j$-th segment,  % $j$, 
and responds with the parity 
(denoted $p^*$)
of all opened bits.
The client reconstructs %$\DB[i]$
the answer 
as $p^* \oplus p_j$.
\end{enumerate}
In this construction, $S = \sqrt{n}$ and $T = \sqrt{n} - 1$.

You might have observed that this upper bound for Yao's
box problem is reminiscient of the  
Piano PIR scheme~\cite{piano}.


\subsection{Lower Bound} 
We now prove \Cref{thm:yaobox}.
The intuition of the proof is 
if we can obtain a too-good-to-be-true  
tradeoff 
between $S$ and $T$, then we can construct 
an encoding of $\DB \in \{0, 1\}^n$ 
whose length is less than $n$, 
which violates 
Shannon's  
fundamental theorem of information theory.
Specifically, we can obtain some compression 
in the encoding 
by leveraging the following fact: every time 
we open $T$ boxes not including $i$, we 
learn not just the values under the $T$ opened boxes 
but also an additional 
bit, namely, the $i$-th bit --- this effectively  
gains us one bit of advantage.

The formal proof works as below.

\begin{proof}[Proof of \Cref{thm:yaobox}]
Suppose we have some protocol for solving Yao's box problem 
with parameters $S$ and $T$. 
We fix all the coins in the protocol denoted ${\sf coins}$,
and we will give ${\sf coins}$ as input to 
both the encoder and decoder. 
As long as $\DB$ is randomly sampled independently of ${\sf coins}$,
we can use Shannon's theorem to argue
that the encoding length must be at least $n$.

\paragraph{Encoding.}
The encoding algorithm is given ${\sf coins}$ and ${\sf DB}$ as input,
and constructs an encoding of ${\sf DB}$ as follows.
%Initially, let ${\sf known} = \emptyset$.

    \begin{enumerate}[leftmargin=6mm]
        \item Seed the protocol (for solving Yao's box problem) 
with ${\sf coins}$, and 
perform preprocessing
such that the client obtains an $S$-bit hint $h$.
        \item 
Initially, let $\known = \{\}$. 
Repeat the following: 
in each time step $i$, the client 
\begin{itemize}[leftmargin=5mm]
\item finds the smallest 
index $q_i \notin \known$, and 
runs the query protocol for index $q_i$; 
\item 
let $\known \leftarrow \known 
\cup \{q_i\} \cup \{\text{all boxes opened during this query}\}$
\item if $\known = [n]$, break. %Otherwise, repeat step 2 again.
\end{itemize}
    \end{enumerate}

Define the {\it newly} opened boxes in 
time step $i$ as follows: 
all boxes opened in time step $i$ that are not in the ${\sf known}$ 
set yet. 
Output an encoding containing the following terms:
\begin{enumerate}[leftmargin=6mm]
\item 
The client's hint $h$;
\item 
Let $V_i$ be the {\it newly} opened values  
(in the order they are opened)
for time step $i$.
Include $V_1, V_2, \ldots, V_k$ 
in the encoding, where $k$ 
is the total number of time steps.
\end{enumerate}

\paragraph{Decoding.}
The decoding algorithm is given ${\sf coins}$
and an encoding $C$ as input, and 
outputs a decoded string as follows.
\begin{enumerate}[leftmargin=6mm]
\item 
Seed the protocol (for solving Yao's box problem)
with ${\sf coins}$.  
Treat
the first $S$ bits of $C$ as the client's hint, 
and in the steps below, consume the rest of $C$ bit by bit 
in a streaming manner.
\item 
Initially, let $\known= \emptyset$.
Every time step $i$, 
\begin{itemize}[leftmargin=5mm] 
\item Let $q_i$ be the smallest
index not in $\known$; 
\item 
The client performs a query for $q_i$ with the server.
Whenever the 
server needs to open some box: if the box was already
opened in some earlier time step, use the same opened value 
as previously learned;  
else if the box has not been opened, 
treat the next bit in $C$ 
as the opened value.
\item 
For all newly opened 
boxes, record the opened values. 
Additionally, reconstruct the value at queried index $q_i$.
\item 
Let $\known \leftarrow \known 
\cup \{q_i\} \cup \{\text{all boxes opened during this query}\}$; 
if $\known = [n]$, break. 
\end{itemize}
\item 
At the end of the protocol, ${\sf known} = [n]$, i.e., 
the values for 
$n$ indices
have been discovered. 
Output this reconstructed string.
\end{enumerate}
\end{proof}
Correctness of the  
decoding is easy to verify: 
the decoding algorithm opens the same sequence
of boxes as the encoding algorithm, and for
every newly opened box, decoding observes the correct
value from $C$.
%as long as the decoding
%algorithm is given the same ${\sf coins}$ 
%as the encoding algorithm, and the 
%same hint $h$ that was generated 
%during the encoding algorithm, the sequence of the boxes
%opened during decoding will be identical 
%to those opened during encoding.



Let $t_1, \ldots, t_k$ be the 
number of {\it newly} opened boxes
in each of the $k$ time steps.
Thus, the number of elements added to $\known$ in time step
$i$
is $t_i + 1$.
We also know that $t_i \leq T$ for all $i \in [k]$, and regardless
of the choice of $\DB$.
Henceforth, let $t = \frac{1}{k}\sum_{i \in [k]} t_i$.
Since the the encoding/decoding algorithm 
stops as soon as ${\sf known} = [n]$, 
it means that $\sum_{i\in [k]}(t_i+1) = (t + 1)\cdot k = n$.
Thus, $k = n/(t+1)$.

The length of the encoding 
is $S + \sum_{i \in [k]}t_i = S + t \cdot k$.
Note that $t$ and $k$ are random variables
that depend on the choice of $\DB$.
By Shannon's theorem, we know that 
\[
\underset{\DB \getr \{0, 1\}^n}{\E} \left[S + t \cdot k  \right] 
= 
\underset{\DB \getr \{0, 1\}^n}{\E} \left[S + t \cdot \frac{n}{t+1}  \right] 
\geq n 
\]
Observe that for any $\DB$,
$\frac{t}{t+1} \leq \frac{T}{T+1}$.
Thus, we have that 
\[S + n \cdot \frac{T}{T+1} \geq n\]
which directly implies that $S(T+1) \geq n$.
% S (T+1) + n \cdot T \geq n * T + n
%
%\elaine{TODO: edit below}



%\label{box_lower}
\ignore{
\begin{theorem}
    $S(T+1) \ge N$ 
\end{theorem}
\begin{proof}
    We will be using an encoding type argument. Consider the following experiment.
    \begin{enumerate}
        \item Run preprocessing
        \item Define an empty set called $\known = \{\}$. In each step $i$, the client finds the smallest $q_i \notin \known$, queries $q_i$.

        With this step, $\known \impliedby \known \cup \{q_i\} \cup {\text{all boxes opened during query}}$
        \item If $\known \neq [N]$, break. Otherwise, repeat step 2 again.
    \end{enumerate}

    Let the ``client's hint" denote whatever information that the client stores after the preprocessing phase. The hint is at most $S$ bits long.
    
    Define the encoding $\enc$ for this process as follows: 
    \[\enc = \text{client's hint} + \text{all ``newly opened" boxes in all queries}\]

    We write ``newly opened" because we do not want to include values that have already been recorded in the encoding.
    \vspace{5mm}
    
    By Shannon's theorem, we will show that $|\enc| \ge N$.

    Let $t_1, t_2, ...\ t_k$ be the number of newly opened boxes at each step $i \in [k]$. 
    
    Note here that we have the power of \textbf{plus one}; if I open $t$ boxes, I end up learning $t+1$ new bits. This is because I also learn the value of the query without opening its box.

    Thus, the $\known$ set increments with the following pattern.

    \begin{itemize}
        \item We newly open $t_1$ boxes: $\known$ increments by $t_1 + 1$
        \item We newly open $t_2$ boxes: $\known$ increments by $t_2 + 1$
        \item and so on...
    \end{itemize}

    Thus, we have that $\sum_{i = 1}^k (t_i + 1)\ge N$. Let $t = \frac{\sum_{i = 1}^k t_i}{k}$. Note that $t$ is the average number of boxes opened on each iteration so it must be upper bounded by $T$, the maximum number of boxes that can be opened in an iteration. Since we repeat the steps until $\known = [N]$, we have that
    \[\sum_{i = 1}^k (t_i + 1)\ge N \implies (t+1)k \ge N \implies k\ge \frac{N}{t+1} \tag{1}\]

    The encoding size is $S + \sum_{i=1}^k t_k = S + tk$. By Shannon's we have that $S + tk \ge N$. By (1), we have that 
    \begin{align*}
        S + tk &\ge N \\
        &\implies S + t \frac{N}{t+1} \ge N \tag{By (1)}\\
        &\implies S(t+1) \ge N \tag{Simplification}\\
        &\implies S(T+1) \ge N \tag{As $t \le T$}
    \end{align*}
\end{proof}
}

\section{Space-Time Tradeoff for Preprocessing PIR}
We will next prove a space-time tradeoff for preprocessing PIR.
Intuitively, the privacy requirement of PIR 
implies that during a query for some index $i \in [n]$,
the probability that the server actually visits position $i$
is rather small.
This allows us to  
rely on the space-time tradeoff for Yao's box problem
to prove a PIR lower bound.


\begin{theorem}[Space-time tradeoff for preprocessing PIR~\cite{CK20}]
    Suppose we have a 1-server prepossessing PIR with perfect correctness and 
${\sf negl}(n)$ privacy loss 
where ${\sf negl}(\cdot)$ denotes a negligible function. 
Let $S$ be the client space and let $T$ be the server computation 
per query.
Then,
    \[(S+1)(T+1) \ge  \Omega(N) \]
\elaine{TODO: change the statement based on the proof}
\label{thm:pir-st}
\end{theorem}
This lower bound holds even for computationally private schemes. It holds even for a single query, regardless of bandwidth, 
number of rounds, 
and preprocessing cost. 
However, we require that the server 
store 
only the original database during the query phase
and no extra information.
%even when preprocessing can be unbounded. 
%The only constraints are that 1) the server
%stores only the original database during the query phase;
%and 2) the client has at most $S$ space during the query phase.   
%However, we have a restriction that the server 
%stores the original database and nothing else. 
%That is, it does not store any encoding of the database.


To prove \Cref{thm:pir-st}, 
we actually need a 
version of Yao's box problem that allows probalistic 
correctness, as stated below. 

\paragraph{Yao's box problem with probabilistic correctness.}
We say that some protocol $\Pi$ solves
Yao's box problem with probabilistic correctness $\delta(n)$,  
iff 
for any $\DB\in \{0, 1\}^n$, 
it holds that 
\[
\Pr\left[
i \getr [n], \text{run $\Pi$ with $\DB$ and $i$} : 
\text{output is correct} 
\right] \geq \delta(n) 
\]

We can extend the proof to Yao's box problem
to allow probabilistic correctness, as stated
in the following theorem:

\begin{theorem}[Yao's box problem with probabilistic correctness]
\elaine{FILL}
\label{thm:yao-prob}
\end{theorem}

Below, we will first assume
that \Cref{thm:yao-prob} 
is true, and prove a lower bound for preprocessing PIR 
in \Cref{sec:pirtoyao}.
Next, in \Cref{sec:prob-yao}, 
we show how to modify the earlier proof for Yao's box problem
to get the probalistic version 
(i.e., \Cref{thm:yao-prob}) 
that we need.


\subsection{Preprocessing PIR  $\Longrightarrow$ Yao} 
\label{sec:pirtoyao}
Given a preprocessing PIR scheme, 
we will construct a solution to Yao's box problem (with probablistic
correctness) as follows:

\begin{itemize}[leftmargin=6mm]
    \item Client's hint: PIR's hint.
    \item Query for $i \in [n]$: Run PIR for query $i$. 
If server is about to look at $\db[i]$, then stop and 
simply output ``error''; else,
output the PIR's reconstructed answer.
\end{itemize}

Clearly, if the PIR scheme has client space $S$
and server time $T$, 
then the resulting protocol (for solving Yao's box problem)
also enjoys client space $S$
and server time as most $T$, where server time 
is measured in terms of the number of locations visited by the server.
Moreover, we claim that the protocol
satisfies the following probabilistic correctness: 


\begin{claim}
Suppose the PIR scheme has perfect
correctness and negligible privacy loss.  
Then, 
the above protocol solves Yao's box problem
with 
correctness probability 
$1 - \frac{T}{n} - \negl(n)$ for a random query.
\end{claim}
\begin{proof}
%% We stop at 46:00 in the recording.
%To show that
%the above solution to Yao's problem
%has correctness probability  
%$1 - \frac{T}{n} - \negl(n)$
%on a random index $i$,  
Henceforth, fix an arbitrary $\DB$. 
Let ${\sf PIRExpt}^{j}$
denote the random experiment 
that runs the PIR scheme over database $\DB$ and query $j$.
let 
\[
p_{j, i} := 
\Pr\left[
\text{ server visits $i$ in ${\sf PIRExpt}^{j}$}
\right]
\]

%In the above protocol that solves Yao's problem, 
%the only situation 
%that breaks correctness is when a query for $i$ causes the server
%to visit $i$.
It suffices to prove 
the following:
\begin{equation}
\Pr\left[
i \getr [n], %\text{run PIR with database $\DB$ and query $i$:}
\text{ server visits $i$ in ${\sf PIRExpt}^{i}$}
\right]
 = \frac{1}{n} \cdot \sum_{i \in [n]}  p_{i, i}
\leq \frac{T}{n} + \negl(n)
\label{eqn:goal}
\end{equation} 

We now prove the above inequality.
The privacy requirement of PIR implies 
that for any $j, i \in [n]$, 
$|p_{j, i} - p_{i, i}| \leq \negl'(n)$.
Since the server can perform at most $T$ amount of work, we have that
for any $ j \in [n]$, $\sum_{i \in [n]} p_{j, i} \leq T$. 
Combining the above facts, we have that 
\[
T \geq 
\sum_{i \in [n]} p_{j, i} 
\geq \sum_{i \in [n]} (p_{i, i} - \negl'(n))
%\leq T
\]
which gives us \Cref{eqn:goal}.
\end{proof}

Combining the above reduction and 
\Cref{thm:yao-prob}, we immediately get the claimed space-time
tradeoff for 1-server preprocessing PIR. 


%We want to show the following. Given that PIRExpt: $i \xleftarrow{\$} [N]$, PIR preprocessing, PIR query on $i$,
%\[p =\prob[\text{PIRExpt opens $i$}] \le \frac{T}{N} + \negl(N)\]




\subsection{Proof of \Cref{thm:yao-prob}} 
\label{sec:prob-yao}
We now prove \Cref{thm:yao-prob}. 
We present a version of the proof described by De, Trevisan, 
and Tulsiani~\cite{DTT10}. 


\begin{fact}[Extension of Shannon's theorem for codes
with probabilistic correctness.]
Suppose there is a randomized encoding procedure
${\sf Enc}: \{0, 1\}^n \times \{0, 1\}^r \rightarrow \{0, 1\}^m$
and decoding procedure 
${\sf Dec}: \{0, 1\}^m \times \{0, 1\}^r \rightarrow \{0, 1\}^n$
such that 
\[
%\Pr_{\substack{r\getr\{0, 1\}^r, \DB \getr \{0, 1\}^n}}\left[
\Pr\left[
r\getr\{0, 1\}^r, \DB \getr \{0, 1\}^n:
{\sf Dec}\left({\sf Enc}(\DB, r), r\right)
= \DB
\right]
\geq \delta
\]
Then, $m \geq n - \log(\frac{1}{\delta})$.
\end{fact}
\begin{proof}
There must exist some $r$ such that 
$\Pr\left[\DB \getr \{0, 1\}^n:
{\sf Dec}\left({\sf Enc}(\DB, r), r\right)
= \DB
\right]\geq \delta$.
In other words, there is an $r$ such that at least $\delta$ 
fraction of the databases can decrypt correctly.
This means that ${\sf Enc}(\DB, r)$
must attain at least 
$\delta \cdot 2^n$ values 
as $\DB$ varies over $\{0, 1\}^n$.
As the total number of values $\Enc(\cdot, r)$ can attain
is at most $2^m$, we have that 
$2^m \geq \delta \cdot 2^n$ which gives us the desired
inequality.
\end{proof}

\begin{lemma}
Consider a {\bf deterministic}
protocol $\Pi$ for solving Yao's box problem with 
client space $S$ and server time $T$. 
Let $\DB \in \{0, 1\}^n$ be some fixed string 
such that 
$\Pi$ gives a correct answer
on  
at least $\frac{1}{2} + \epsilon$ 
fraction of the indices.
Then, 
%there is a randomized
%encoding ${\sf Enc}$ and decoding procedure 
%${\sf Dec}$ such that 
${\sf Enc}: \{0, 1\}^n \times \{0, 1\}^r \rightarrow \{0, 1\}^m$
and decoding procedure
${\sf Dec}: \{0, 1\}^m \times \{0, 1\}^r \rightarrow \{0, 1\}^n$
such that
\[
\Pr\left[r \getr \{0, 1\}^r: 
{\sf Dec}\left({\sf Enc}(\DB, r), r\right) 
\right]
\geq 
\Omega(\epsilon/T)
\]
Further, the encoding length 
$m \leq n - \frac{\epsilon^2 n}{10T} + S + O(1)$.
\end{lemma}
\begin{proof}
\elaine{FILL}
\end{proof}


\section*{Old TEXT below}
\vspace{5mm}

For the proof, we will show that a solution to the PIR problem can be used to construct an algorithm to solve a probabilistic version of Yao's Box Problem.

\paragraph{Probabilistic Yao's Box Problem}
Suppose we have a working PIR scheme. Now, we will construct a solution to probabilistic Yao's Box Problem as follows:

\begin{itemize}
    \item Client's Hint: PIR's hint
    \item Query for $i \in [N]$: Run PIR for query $i$. If server looks at $\db[i]$, then output ``error".
\end{itemize}

% We stop at 46:00 in the recording.
We want to show the following. Given that PIRExpt: $i \xleftarrow{\$} [N]$, PIR preprocessing, PIR query on $i$,
\[p =\prob[\text{PIRExpt opens $i$}] \le \frac{T}{N} + \negl(N)\]

That is, we want to show that if we run a PIR query with the a random index $i$, the probability we open that index is small.

For a fixed $i$, define the following probability $p_i = \prob[\text{PIR on $i$ looks at $i$}].$ Then, 
\[p =\frac{1}{N} \sum_i p^i\]

Assume for the sake of contradiction that  $p > \frac{T}{N} + \mu$, where $\mu$ is non-negligible.

Let $p_{ji} = \prob[\text{PIR on $j$ opens $i$}]$. Since our scheme is private, the different indices should be computationally indistinguishable. Thus, this probability should be equally distributed. As a result,
\[p_{ji} = \prob[\text{PIR on $j$ opens $i$}] \ge p_i - \negl(N)\]

\begin{align*}
    E[\text{server work for PIR on $j$}] &\ge \sum_{i=1}^N p_{ji} \\
    &\ge \sum_{i=1}^N (p_{i} - \negl(N))\\
    &= Np - \negl(N) \tag{By def. of $p$}\\
    &> T + \mu N - \negl(N) \tag{As $p > \frac{T}{N} + \mu$}\\ 
\end{align*}

Thus, we have a contradiction because the expected number of locations the server needs to look at is strictly greater than $T$. Thus, we have shown that 

\[\prob[\text{PIRExpt opens $i$}] \le \frac{T}{N} + \negl(N)\]

\vspace{5mm}

Shifting our focus back to Yao's Box problem with probabilistic correctness on random index, the probabilistic correctness is
\[\prob[i \xleftarrow{\$} \text{correct for $i$}] \ge 1 - \frac{T}{N} - \negl(N)\]

\paragraph{Encoding Argument}
Randomness comes from two parts: the preprocessing part (the client's hint), and the query part. With this in mind, we will be using an augmented version of the encoding type argument in \ref{box_lower}.

Consider the following experiment.
\begin{enumerate}
    \item Run preprocessing, and choose a ``reasonably good hint." Initially, let the encoding be just the hint. We will add to this encoding as we go forward.
    \item Define an empty set called $\known = \{\}$
    \item In each step $i$, find the smallest $q_i \notin \known$. 
    \begin{itemize}
        \item If $\exists$ online coins such that query $q_i$ will give the correct answer, choose the lexicographically smallest coin. Execute query.
        \[\known \impliedby \cup \{q_i\} \cup \{\text{all newly opened}\}\]
        Add ``newly opened" to encoding.
        \item Else add $q_i$-th bit to the encoding.
    \end{itemize}
    \item Repeat until $\known = [N]$.
    
\end{enumerate}

A hint is bad for $i \in [N]$ if $\prob[\text{query $i$ correct} | \text{hint}] = 0$. That is, there does not exist an online coin such that the query is correct.

\begin{claim}
    $\exists$ hint that's bad for at most $T + 1$ location.
\end{claim}
\begin{proof}
    If all hints are bad for  more than $T+1$ locations, then
    \[\prob[i \in [N], \text{correct on $i$}] < 1 - \frac{T+1}{N}\]

    We have a contradiction, because it disagrees with our probabilistic correctness result above.
\end{proof}

Finally, we can reason about the encoding length. 

Suppose the worst case where the hint is bad for exactly $T+1$ locations. Let $\bbad$ be the encoding of the ``newly opened" boxes in the bad queries, and $\bgood$ be the encoding of the ``newly opened boxes in the good queries.
\begin{itemize}
    \item $|\bbad| = T + 1$, as each bad iteration adds one bit, and we have $T+1$ iterations.
    \item To find $|\bgood|$, we repeat the argument from \ref{box_lower}. Let $t_1, t_2, ...\ t_k$ be the number of newly opened boxes at each good step $i \in [k]$.

    By the ``plus one" argument, we have that $\sum_{i = 1}^k (t_i + 1)\ge N - (T + 1)$. We subtract $T + 1$ here because those indices have been handled by the bad iterations.
    
    Let $t = \frac{\sum_{i = 1}^k t_i}{k}$. Then as before, we have $k\ge \frac{N}{t+1}$
    \begin{align*}
        \sum_{i = 1}^k (t_i + 1)&\ge N - (T + 1) \\
        &\implies tk + k \ge N - T - 1 \\
        &\implies k \ge \frac{N - T - 1}{t+1}
    \end{align*}

    Thus, $|\bgood| = tk \ge t\frac{N - T - 1}{t+1}$
\end{itemize}

With the information above, we can mek the following conclusion.
\begin{align*}
    |\enc| &= S + |\bgood| + |\bbad| \ge N \\
    &\implies S + t\frac{N - T - 1}{t+1} + T + 1 \ge N \\
    &\implies S(t+1) + T + 1 \ge N \tag{By simplification}\\
    &\implies (S+1)(T+1) \ge N \tag{As $t$ upper bounded by $T$}
\end{align*}

\qed
