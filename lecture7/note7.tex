%\usepackage{xspace}

\newcommand{\bits}{\{0,1\}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfv}{\mathbf{v}}
\newcommand{\bfp}{\mathbf{p}}
\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfy}{\mathbf{y}}

\newcommand{\calR}{\mathcal{R}}

\newcommand{\CNextMsg}{\ensuremath{{\sf C Next Msg}}}
\newcommand{\SNextMsg}{\ensuremath{{\sf S Next Msg}}}
\newcommand{\CNext}{\ensuremath{{\sf C Next}}}
\newcommand{\SNext}{\ensuremath{{\sf S Next}}}
\newcommand{\Cstp}{\ensuremath{{\sf Cst'}}}
\newcommand{\Cst}{\ensuremath{{\sf Cst}}}
\newcommand{\msg}{\ensuremath{{\sf msg}}}
\newcommand{\msgp}{\ensuremath{{\sf msg'}}}
\newcommand{\Coutput}{\ensuremath{{\sf Reconstr}}}
\newcommand{\ans}{\ensuremath{{\sf ans}}}
\newcommand{\Ccoins}{\ensuremath{{\sf Ccoins}}}
\newcommand{\Scoins}{\ensuremath{{\sf Scoins}}}
%\newcommand{\Comm}{\ensuremath{{\sf Comm}}}
\newcommand{\Expt}{\ensuremath{{\sf Expt}}}
\newcommand{\coin}{\ensuremath{{\sf coin}}}
\newcommand{\View}{\ensuremath{{\sf View}}}
%\newcommand{\negl}{\ensuremath{{\sf negl}}}
\newcommand{\PPT}{PPT }
\newcommand{\Out}{\ensuremath{{\sf Out}}}
\newcommand{\OWF}{\ensuremath{{\sf OWF}}}
\newcommand{\OT}{\ensuremath{{\sf OT}}}
\newcommand{\PIR}{\ensuremath{{\sf PIR}}}
\newcommand{\Server}{\ensuremath{{\sf Server}}}
\newcommand{\Client}{\ensuremath{{\sf Client}}}
\newcommand{\Alice}{\ensuremath{{\sf Alice}}}
\newcommand{\Bob}{\ensuremath{{\sf Bob}}}
%\newcommand{\getr}{\ensuremath{~{\overset{\$}{\leftarrow}}}~}
\newcommand{\get}{\ensuremath{\leftarrow}}
\newcommand{\E}{\ensuremath{{\bf E}}}
\newcommand{\out}{\ensuremath{{\sf out}}}
%\newcommand{\PRF}{\ensuremath{{\sf PRF}}}
\newcommand{\prob}{\ensuremath{\mathbb{P}}}
\newcommand{\known}{\ensuremath{{\sf known}}}
\newcommand{\enc}{\ensuremath{{\sf enc}}}
\newcommand{\db}{\ensuremath{{\sf DB}}}
\newcommand{\bbad}{\ensuremath{{\sf b_{bad}}}}
\newcommand{\bgood}{\ensuremath{{\sf b_{good}}}}

%\setlength{\parindent}{0pt}

%This is our final lecture on private information retrieval. 
In this lecture, we will prove a lower bound about the client space and server computation 
tradeoff for preprocessing PIR schemes. 
%For the proof, we will use techniques from complexity theory literature.
We will borrow techniques for proving time-space tradeoff
from the complexity theory literature.

Specifically, we consider a 1-server preprocessing PIR scheme 
with the following syntax:
\begin{itemize}[leftmargin=7mm]
\item {\it Preprocessing algorithm.}
Suppose there is a (possibly randomized and unbounded) %, possibly randomized
preprocessing function denoted 
${\sf Prep} : \{0, 1\}^n \rightarrow \{0, 1\}^S$ 
that takes in an $n$-bit database $\DB\in\{0, 1\}^n$ as input,
and outputs an $S$-bit hint string denoted $h$.
\item 
{\it A single query.} 
The client and the server perform a (possibly randomized) query protocol.  
The client takes in $h$ and some index $i \in [n]$ as input, and the 
server takes 
the database $\DB\in \{0, 1\}^n$ as input. 
%Then, the client engages in a query protocol with the server,
%to learn the database at some index  
%$i \in [n]$.
To answer the query, the server is allowed to read at most $T$ locations of the database. 
\end{itemize}
In other words, we do not place any restriction
on the amount of work performed
during preprocessing. The only constraints
are that at the end of the preprocessing: 
1) the client  
is allowed to store only the hint $h$; and 2) the server
is allowed to store only the original database $\DB$ and no extra information.

%Both the preprocessing function ${\sf Prep}$
%and the query protocol can be randomized. 
We assume perfect correctness, i.e., for any $\DB \in \{0, 1\}^n$, any query $i \in [n]$, 
correctness holds with probability $1$.
Let ${\sf view}_S(\DB, i)$ 
denote the server's view during the query phase 
when we run the PIR scheme (i.e., preprocessing followed by the query protocol)
over inputs $\DB$ and $i$.
For security, we require 
that for any $\DB \in \{0, 1\}^n$, any $i, j \in [n]$,  
${\sf view}_S(\DB, i) \approx {\sf view}_S(\DB, j)$
where $\approx$ means computational indistinguishability.

For such a preprocessing PIR scheme, we will prove a tradeoff
between $S$ and $T$ as stated in the following theorem: 


\begin{theorem}[Time space tradeoff for preprocessing PIR~\cite{CK20}]
    Given a 1-server preprocessing PIR, 
        let $S$ be the client space, and let $T$ be the server compuation per query. 
    Then,  
    $(S+1)(T+1) \ge N$. \elaine{TODO: edit the theorem based on what we can prove later}
\label{thm:lb}
\end{theorem}

Again, note that the lower bound 
holds even when we allow the preprocessing to be unbounded, 
even when there is only a single query
after the preprocessing, 
and even when 
the query phase may have arbitrarily many rounds of interaction. 



\paragraph{Piano.}
Recall that in an earlier lecture, 
we covered Piano~\cite{piano}, a preprocessing 1-server PIR scheme. 
%It uses the client-specific preprocessing model, meaning that each client has a subscription phase with the server, during which it will perform preprocessing. 
Piano enjoys 
the following performance bounds:
\begin{itemize}[leftmargin=7mm]
    \item Client space: $\widetilde{O}(\sqrt{n})$
where $\widetilde{O}(\cdot)$ hides a(n  arbitrarily small) superlogarithmic function.
    \item Communication per query: $O(\sqrt{n})$
    \item Server computation per query: ${O}(\sqrt{n})$
\end{itemize}

We can see that Piano achieves optimal 
client space and server computation tradeoff (up to polylogarithmic factors)
in light of \Cref{thm:lb}.
%Consider the server computation and client space trade-off. For PIANO, this trade-off [Client Space: $\widetilde{O}(\sqrt{n})$, Compute per Query: $\widetilde{O}(\sqrt{n})$] is nearly optimal up to poly-log factors. 


%This is because of the following lower bound.
%\paragraph{Space/Computation Lower Bound}


%This lower bound holds even for a scheme that supports 1 query, regardless of server space, bandwidth, and amount of preprocessing work. 

\section{Warmup: Yao's Box Problem}
\label{sec:yao}
Before proving \Cref{thm:lb}, 
we first prove a time-space tradeoff for a classical problem
called the Yao's box problem~\cite{yao}, and we shall
see why 
Yao's box problem is closely related to preprocessing PIR.

We have a server with $n$ boxes, each covering a bit. 
Henceforth we use $\DB := (\DB[1], \ldots, \DB[n])$ to denote the $n$ bits.
Consider the following game:
\begin{itemize}[leftmargin=5mm]
\item 
{\it Preprocessing.}
During a preprocessing phase, the server and client can 
perform an unbounded amount of computation. At the end of
preprocessing, the client obtains an $S$-bit hint $h$;
the server does not store any extra information
besides $\DB$ itself where each bit is covered by some box. 
\item 
{\it Query.}
The client wants to find out  
$\DB[i]$ for some index $i \in [n]$.
The client and server 
now engage in some protocol at the end of which the client outputs
an answer $\beta$.
The query protocol must satisfy two requirements:
1) the server is {\it not allowed to open box $i$}
during the protocol; and 2) the server 
can open at most $T$ boxes.
\end{itemize}

We allow the preprocessing algorithm and query 
protocol to be possibly randomized.
We require that the protocol to have {\it perfect correctness},
i.e., for any $\DB \in \{0, 1\}^n$, 
any index $i \in [n]$, 
after performing the preprocessing and 
the query protocol for index $i$, the client's output $\beta = \DB[i]$ with 
probability $1$. 


Note that unlike PIR, 
Yao's box problem does not have any privacy requirement.
In particular, it is perfectly ok for the query 
index $i$ to be leaked
to the server.



\ignore{
\paragraph{Preprocessing Phase}
Client and server can open all bits, and run arbitrary and unbounded computations. However, we have a constraint that at the end of the preprocessing, client can only store $S$ bits of information.

\paragraph{Query}
Client wants to know the i-th bit. We allow the client and server to have unbounded communication and work. We have a constraint that the server can open at most $T$ boxes and it cannot open box $i$.
}

In the above formulation, $S$ denotes the client space
at the end of preprocessing, and $T$ can be viewed as a lower bound on the 
server's running time during the query phase. 
We care about characterizing the tradeoff between $S$ and $T$,
that is, the client-space and server-time tradeoff.

Yao~\cite{yao} proved the following theorem:

\begin{theorem}[Yao's box problem] 
For any protocol that solves Yao's box problem, it must be that 
$S \cdot (T+1) \geq n$, 
where $S$ is the client space
and $T$ is the maximum number of boxes opened during the query. 
%\elaine{TODO: modify the statement
%based on proof}  
\label{thm:yaobox}
\end{theorem} 


\subsection{Upper Bound}
Before proving \Cref{thm:yaobox}, 
let us first see a simple upper bound that can match 
the $S\cdot (T+1) = n$ tradeoff.
For simplicity, assume $n$ is a perfect square.
\begin{enumerate}
    \item Divide the $n$ boxes into $\sqrt{n}$ segments each
of size $\sqrt{n}$. 
    \item Preprocessing: 
the client stores the parity of each segment, denoted $p_1, \ldots, p_{\sqrt{n}}$
respectively.
    \item Query for index $i \in [n]$: 
suppose $i$ belongs 
to the $j$-th segment. 
The server opens 
every other box 
except $i$ in the $j$-th segment,  % $j$, 
and responds with the parity 
(denoted $p^*$)
of all opened bits.
The client reconstructs %$\DB[i]$
the answer 
as $p^* \oplus p_j$.
\end{enumerate}
In this construction, $S = \sqrt{n}$ and $T = \sqrt{n} - 1$.

You might have observed that this upper bound for Yao's
box problem is reminiscient of the  
Piano PIR scheme~\cite{piano}.


\subsection{Lower Bound} 
We now prove \Cref{thm:yaobox}.
The intuition of the proof is 
if we can obtain a too-good-to-be-true  
tradeoff 
between $S$ and $T$, then we can construct 
an encoding of $\DB \in \{0, 1\}^n$ 
whose length is less than $n$, 
which violates 
Shannon's  
fundamental theorem of information theory.
Specifically, we can obtain some compression 
in the encoding 
by leveraging the following fact: every time 
we open $T$ boxes not including $i$, we 
learn not just the values under the $T$ opened boxes 
but also an additional 
bit, namely, the $i$-th bit --- this effectively  
gains us one bit of advantage.

The formal proof works as below.

\begin{proof}[Proof of \Cref{thm:yaobox}]
Suppose we have some protocol for solving Yao's box problem 
with parameters $S$ and $T$. 
We fix all the coins in the protocol denoted ${\sf coins}$,
and we will give ${\sf coins}$ as input to 
both the encoder and decoder. 
As long as $\DB$ is randomly sampled independently of ${\sf coins}$,
we can use Shannon's theorem to argue
that the encoding length must be at least $n$.

\paragraph{Encoding.}
The encoding algorithm is given ${\sf coins}$ and ${\sf DB}$ as input,
and constructs an encoding of ${\sf DB}$ as follows.
%Initially, let ${\sf known} = \emptyset$.

    \begin{enumerate}[leftmargin=6mm]
        \item Seed the protocol (for solving Yao's box problem) 
with ${\sf coins}$, and 
perform preprocessing
such that the client obtains an $S$-bit hint $h$.
        \item 
Initially, let $\known = \{\}$. 
Repeat the following: 
in each time step $i$, the client 
\begin{itemize}[leftmargin=5mm]
\item finds the smallest 
index $q_i \notin \known$, and 
runs the query protocol for index $q_i$; 
\item 
let $\known \leftarrow \known 
\cup \{q_i\} \cup \{\text{all boxes opened during this query}\}$
\item if $\known = [n]$, break. %Otherwise, repeat step 2 again.
\end{itemize}
    \end{enumerate}

Define the {\it newly} opened boxes in 
time step $i$ as follows: 
all boxes opened in time step $i$ that are not in the ${\sf known}$ 
set yet. 
Output an encoding containing the following terms:
\begin{enumerate}[leftmargin=6mm]
\item 
The client's hint $h$;
\item 
Let $V_i$ be the {\it newly} opened values  
(in the order they are opened)
for time step $i$.
Include $V_1, V_2, \ldots, V_k$ 
in the encoding, where $k$ 
is the total number of time steps.
\end{enumerate}

\paragraph{Decoding.}
The decoding algorithm is given ${\sf coins}$
and an encoding $C$ as input, and 
outputs a decoded string as follows.
\begin{enumerate}[leftmargin=6mm]
\item 
Seed the protocol (for solving Yao's box problem)
with ${\sf coins}$.  
Treat
the first $S$ bits of $C$ as the client's hint, 
and in the steps below, consume the rest of $C$ bit by bit 
in a streaming manner.
\item 
Initially, let $\known= \emptyset$.
Every time step $i$, 
\begin{itemize}[leftmargin=5mm] 
\item Let $q_i$ be the smallest
index not in $\known$; 
\item 
The client performs a query for $q_i$ with the server.
Whenever the 
server needs to open some box: if the box was already
opened in some earlier time step, use the same opened value 
as previously learned;  
else if the box has not been opened, 
treat the next bit in $C$ 
as the opened value.
\item 
For all newly opened 
boxes, record the opened values. 
Additionally, reconstruct the value at queried index $q_i$.
\item 
Let $\known \leftarrow \known 
\cup \{q_i\} \cup \{\text{all boxes opened during this query}\}$; 
if $\known = [n]$, break. 
\end{itemize}
\item 
At the end of the protocol, ${\sf known} = [n]$, i.e., 
the values for 
$n$ indices
have been discovered. 
Output this reconstructed string.
\end{enumerate}
\end{proof}
Correctness of the  
decoding is easy to verify: 
the decoding algorithm opens the same sequence
of boxes as the encoding algorithm, and for
every newly opened box, decoding observes the correct
value from $C$.
%as long as the decoding
%algorithm is given the same ${\sf coins}$ 
%as the encoding algorithm, and the 
%same hint $h$ that was generated 
%during the encoding algorithm, the sequence of the boxes
%opened during decoding will be identical 
%to those opened during encoding.



Let $t_1, \ldots, t_k$ be the 
number of {\it newly} opened boxes
in each of the $k$ time steps.
Thus, the number of elements added to $\known$ in time step
$i$
is $t_i + 1$.
We also know that $t_i \leq T$ for all $i \in [k]$, and regardless
of the choice of $\DB$.
Henceforth, let $t = \frac{1}{k}\sum_{i \in [k]} t_i$.
Since the the encoding/decoding algorithm 
stops as soon as ${\sf known} = [n]$, 
it means that $\sum_{i\in [k]}(t_i+1) = (t + 1)\cdot k = n$.
Thus, $k = n/(t+1)$.

The length of the encoding 
is $S + \sum_{i \in [k]}t_i = S + t \cdot k$.
Note that $t$ and $k$ are random variables
that depend on the choice of $\DB$.
By Shannon's theorem, we know that 
\[
\underset{\DB \getr \{0, 1\}^n}{\E} \left[S + t \cdot k  \right] 
= 
\underset{\DB \getr \{0, 1\}^n}{\E} \left[S + t \cdot \frac{n}{t+1}  \right] 
\geq n 
\]
Observe that for any $\DB$,
$\frac{t}{t+1} \leq \frac{T}{T+1}$.
Thus, we have that 
\[S + n \cdot \frac{T}{T+1} \geq n\]
which directly implies that $S(T+1) \geq n$.
% S (T+1) + n \cdot T \geq n * T + n
%
%\elaine{TODO: edit below}



%\label{box_lower}
\ignore{
\begin{theorem}
    $S(T+1) \ge N$ 
\end{theorem}
\begin{proof}
    We will be using an encoding type argument. Consider the following experiment.
    \begin{enumerate}
        \item Run preprocessing
        \item Define an empty set called $\known = \{\}$. In each step $i$, the client finds the smallest $q_i \notin \known$, queries $q_i$.

        With this step, $\known \impliedby \known \cup \{q_i\} \cup {\text{all boxes opened during query}}$
        \item If $\known \neq [N]$, break. Otherwise, repeat step 2 again.
    \end{enumerate}

    Let the ``client's hint" denote whatever information that the client stores after the preprocessing phase. The hint is at most $S$ bits long.
    
    Define the encoding $\enc$ for this process as follows: 
    \[\enc = \text{client's hint} + \text{all ``newly opened" boxes in all queries}\]

    We write ``newly opened" because we do not want to include values that have already been recorded in the encoding.
    \vspace{5mm}
    
    By Shannon's theorem, we will show that $|\enc| \ge N$.

    Let $t_1, t_2, ...\ t_k$ be the number of newly opened boxes at each step $i \in [k]$. 
    
    Note here that we have the power of \textbf{plus one}; if I open $t$ boxes, I end up learning $t+1$ new bits. This is because I also learn the value of the query without opening its box.

    Thus, the $\known$ set increments with the following pattern.

    \begin{itemize}
        \item We newly open $t_1$ boxes: $\known$ increments by $t_1 + 1$
        \item We newly open $t_2$ boxes: $\known$ increments by $t_2 + 1$
        \item and so on...
    \end{itemize}

    Thus, we have that $\sum_{i = 1}^k (t_i + 1)\ge N$. Let $t = \frac{\sum_{i = 1}^k t_i}{k}$. Note that $t$ is the average number of boxes opened on each iteration so it must be upper bounded by $T$, the maximum number of boxes that can be opened in an iteration. Since we repeat the steps until $\known = [N]$, we have that
    \[\sum_{i = 1}^k (t_i + 1)\ge N \implies (t+1)k \ge N \implies k\ge \frac{N}{t+1} \tag{1}\]

    The encoding size is $S + \sum_{i=1}^k t_k = S + tk$. By Shannon's we have that $S + tk \ge N$. By (1), we have that 
    \begin{align*}
        S + tk &\ge N \\
        &\implies S + t \frac{N}{t+1} \ge N \tag{By (1)}\\
        &\implies S(t+1) \ge N \tag{Simplification}\\
        &\implies S(T+1) \ge N \tag{As $t \le T$}
    \end{align*}
\end{proof}
}

\section{Space-Time Tradeoff for Preprocessing PIR}
We will next prove a space-time tradeoff for preprocessing PIR.
Intuitively, the privacy requirement of PIR 
implies that during a query for some index $i \in [n]$,
the probability that the server actually visits position $i$
is rather small.
This allows us to  
rely on the space-time tradeoff for Yao's box problem
to prove a PIR lower bound.


\begin{theorem}[Space-time tradeoff for preprocessing PIR~\cite{CK20}]
    Suppose we have a 1-server prepossessing PIR with perfect correctness and 
${\sf negl}(n)$ privacy loss 
where ${\sf negl}(\cdot)$ denotes a negligible function. 
Let $S$ be the client space and let $T$ be the server computation 
per query.
Then,
    \[(S+1)(T+1) \ge  \Omega(N) \]
\elaine{TODO: change the statement based on the proof}
\label{thm:pir-st}
\end{theorem}
This lower bound holds even for computationally private schemes. It holds even for a single query, regardless of bandwidth, 
number of rounds, 
and preprocessing cost. 
However, we require that the server 
store 
only the original database during the query phase
and no extra information.
%even when preprocessing can be unbounded. 
%The only constraints are that 1) the server
%stores only the original database during the query phase;
%and 2) the client has at most $S$ space during the query phase.   
%However, we have a restriction that the server 
%stores the original database and nothing else. 
%That is, it does not store any encoding of the database.


To prove \Cref{thm:pir-st}, 
we actually need a 
version of Yao's box problem that allows probalistic 
correctness, as stated below. 

\paragraph{Yao's box problem with probabilistic correctness.}
We say that some protocol $\Pi$ solves
Yao's box problem with probabilistic correctness $\delta(n)$,  
iff 
for any $\DB\in \{0, 1\}^n$, 
it holds that 
\[
\Pr\left[
i \getr [n], \text{run $\Pi$ with $\DB$ and $i$} : 
\text{output is correct} 
\right] \geq \delta(n) 
\]

We can extend the proof to Yao's box problem
to allow probabilistic correctness, as stated
in the following theorem:

\begin{theorem}[Yao's box problem with probabilistic correctness]
\elaine{FILL}
\label{thm:yao-prob}
\end{theorem}

Below, we will first assume
that \Cref{thm:yao-prob} 
is true, and prove a lower bound for preprocessing PIR 
in \Cref{sec:pirtoyao}.
Next, in \Cref{sec:prob-yao}, 
we show how to modify the earlier proof for Yao's box problem
to get the probalistic version 
(i.e., \Cref{thm:yao-prob}) 
that we need.


\subsection{Preprocessing PIR  $\Longrightarrow$ Yao} 
\label{sec:pirtoyao}
Given a preprocessing PIR scheme, 
we will construct a solution to Yao's box problem (with probablistic
correctness) as follows:

\begin{itemize}[leftmargin=6mm]
    \item Client's hint: PIR's hint.
    \item Query for $i \in [n]$: Run PIR for query $i$. 
If server is about to look at $\db[i]$, then stop and 
simply output ``error''; else,
output the PIR's reconstructed answer.
\end{itemize}

Clearly, if the PIR scheme has client space $S$
and server time $T$, 
then the resulting protocol (for solving Yao's box problem)
also enjoys client space $S$
and server time as most $T$, where server time 
is measured in terms of the number of locations visited by the server.
Moreover, we claim that the protocol
satisfies the following probabilistic correctness: 


\begin{claim}
Suppose the PIR scheme has perfect
correctness and negligible privacy loss.  
Then, 
the above protocol solves Yao's box problem
with 
correctness probability 
$1 - \frac{T}{n} - \negl(n)$ for a random query.
\end{claim}
\begin{proof}
%% We stop at 46:00 in the recording.
%To show that
%the above solution to Yao's problem
%has correctness probability  
%$1 - \frac{T}{n} - \negl(n)$
%on a random index $i$,  
Henceforth, fix an arbitrary $\DB$. 
Let ${\sf PIRExpt}^{j}$
denote the random experiment 
that runs the PIR scheme over database $\DB$ and query $j$.
let 
\[
p_{j, i} := 
\Pr\left[
\text{ server visits $i$ in ${\sf PIRExpt}^{j}$}
\right]
\]

%In the above protocol that solves Yao's problem, 
%the only situation 
%that breaks correctness is when a query for $i$ causes the server
%to visit $i$.
It suffices to prove 
the following:
\begin{equation}
\Pr\left[
i \getr [n], %\text{run PIR with database $\DB$ and query $i$:}
\text{ server visits $i$ in ${\sf PIRExpt}^{i}$}
\right]
 = \frac{1}{n} \cdot \sum_{i \in [n]}  p_{i, i}
\leq \frac{T}{n} + \negl(n)
\label{eqn:goal}
\end{equation} 

We now prove the above inequality.
The privacy requirement of PIR implies 
that for any $j, i \in [n]$, 
$|p_{j, i} - p_{i, i}| \leq \negl'(n)$.
Since the server can perform at most $T$ amount of work, we have that
for any $ j \in [n]$, $\sum_{i \in [n]} p_{j, i} \leq T$. 
Combining the above facts, we have that 
\[
T \geq 
\sum_{i \in [n]} p_{j, i} 
\geq \sum_{i \in [n]} (p_{i, i} - \negl'(n))
%\leq T
\]
which gives us \Cref{eqn:goal}.
\end{proof}

Combining the above reduction and 
\Cref{thm:yao-prob}, we immediately get the claimed space-time
tradeoff for 1-server preprocessing PIR. 


%We want to show the following. Given that PIRExpt: $i \xleftarrow{\$} [N]$, PIR preprocessing, PIR query on $i$,
%\[p =\prob[\text{PIRExpt opens $i$}] \le \frac{T}{N} + \negl(N)\]




\subsection{Proof of \Cref{thm:yao-prob}} 
\label{sec:prob-yao}
We now prove \Cref{thm:yao-prob}. 
We present a variant of the proof\footnote{The proof 
we present below fixes
a bug in their proof. %of Lemma 8.4.
Specifically, the way they take Chernoff bound in the proof of Lemma 8.4 
is incorrect
since the underlying random variables are positively correlated.
Thanks to Ashrujit Ghoshal
for helping me fix the bug.
} 
described by De, Trevisan, 
and Tulsiani~\cite{DTT10}. 
The proof requires somewhat more technical
calculations 
in comparison with the 
earlier proof 
in \Cref{sec:yao}; however, the high-level intuition is still
the same as before. 




\begin{fact}[Extension of Shannon's theorem for codes
with probabilistic correctness.]
Suppose there is a randomized encoding procedure
${\sf Enc}: \{0, 1\}^n \times \{0, 1\}^r \rightarrow \{0, 1\}^m$
and decoding procedure 
${\sf Dec}: \{0, 1\}^m \times \{0, 1\}^r \rightarrow \{0, 1\}^n$
such that 
\[
%\Pr_{\substack{r\getr\{0, 1\}^r, \DB \getr \{0, 1\}^n}}\left[
\Pr\left[
r\getr\{0, 1\}^r, \DB \getr \{0, 1\}^n:
{\sf Dec}\left({\sf Enc}(\DB, r), r\right)
= \DB
\right]
\geq \delta
\]
Then, $m \geq n - \log(\frac{1}{\delta})$.
\end{fact}
\begin{proof}
There must exist some $r$ such that 
$\Pr\left[\DB \getr \{0, 1\}^n:
{\sf Dec}\left({\sf Enc}(\DB, r), r\right)
= \DB
\right]\geq \delta$.
In other words, there is an $r$ such that at least $\delta$ 
fraction of the databases can decrypt correctly.
This means that ${\sf Enc}(\DB, r)$
must attain at least 
$\delta \cdot 2^n$ values 
as $\DB$ varies over $\{0, 1\}^n$.
As the total number of values $\Enc(\cdot, r)$ can attain
is at most $2^m$, we have that 
$2^m \geq \delta \cdot 2^n$ which gives us the desired
inequality.
\end{proof}


\begin{fact}
Let $\epsilon \in (0, 1/2)$. 
Given a set of $M$ items, 
pick a subset of size at most $(1/2 - \epsilon)M$. 
The subset 
can be encoded using $M \cdot (1- 2.87 \epsilon^2)$ bits. 
\label{fct:encodeset}
\end{fact}
\begin{proof}
The number of ways to choose  
a subset of size exactly $(1/2 - \epsilon)M$
among $M$ elements is 
\begin{align*}
 f = & {M \choose (\frac12 - \epsilon)M }
= \frac{M!}{\left((\frac12 - \epsilon)M\right)! \cdot \left((\frac12 + \epsilon)M\right)!}\\
 \leq & \frac{\left(\frac{M}{e}\right)^M}{\left(\frac{(\frac12-\epsilon)M}{e}
\right)^{(\frac12-\epsilon)M} \cdot \left(\frac{(\frac12+\epsilon)M}{e}
\right)^{(\frac12+\epsilon)M}} & \text{(By Stirling)}\\  
 = & \left(\frac{1}{\frac14- \epsilon^2}\right)^{(\frac12 - \epsilon)M} 
\cdot 
\left(\frac{1}{\frac12 + \epsilon}\right)^{2\epsilon M}
\end{align*}
Taking $\log_2 $ of the 
above expression, 
and then using Taylor expansion, we get that 
for sufficiently large $M$, 
$\log_2(f) \leq 
\log_2\left(
\left(\frac{1}{\frac14- \epsilon^2}\right)^{(\frac12 - \epsilon)M}
\cdot
\left(\frac{1}{\frac12 + \epsilon}\right)^{2\epsilon M}
\right) \leq M \cdot (1 - 2.87\epsilon^2)$.
\end{proof}


\begin{lemma}
Consider a {\bf deterministic}
protocol $\Pi$ for solving Yao's box problem with 
client space $S$ and server time $T$, and suppose
$T \in [2, o(n)]$.
Let $\DB \in \{0, 1\}^n$ be some fixed string 
such that 
$\Pi$ gives a correct answer
on  
at least $\frac{3}{4}$ 
\elaine{note param}
fraction of the indices.
\elaine{may need some assumption T much smalller than n}
Then, 
%there is a randomized
%encoding ${\sf Enc}$ and decoding procedure 
%${\sf Dec}$ such that 
${\sf Enc}: \{0, 1\}^n \times \{0, 1\}^r \rightarrow \{0, 1\}^m$
and decoding procedure
${\sf Dec}: \{0, 1\}^m \times \{0, 1\}^r \rightarrow \{0, 1\}^n$
such that
\[
\Pr\left[r \getr \{0, 1\}^r: 
{\sf Dec}\left({\sf Enc}(\DB, r), r\right) 
\right]
\geq 
\Omega(1)
\]
\elaine{FILL some concrete number}
Further, the encoding length 
$m \leq n - \Theta(\frac{n}{T}) + S$.
\end{lemma}
\begin{proof}
%\elaine{FILL}
Suppose we sample each index in $[n]$ with probability $1/10T$
into some random set $R$.
For some $i \in R$, if running the protocol $\Pi$   
on $i$ 
does not end up visiting any position 
in $R$, then we say that $i$ is {\it good}.
Let $G \subseteq R$ be the subset of $R$ that are good. 
We partition $G = G_0 \cup G_1$ into two disjoint subsets,
where $G_0$ includes every index $i \in G$
such that running $\Pi$ on $i$ gives a correct answer,
and  
$G_1$ includes every index $i \in G$
such that running $\Pi$ on $i$ gives an incorrect answer.

\begin{fact}
$\E[|R|] = n/10T$. Further, 
by Chernoff bound, $\Pr[|R| \leq 0.09n/T] \geq \exp(-\Omega(n/T))$.
\label{fct:Rsize}
\end{fact}

\begin{claim}
%$\E[|G|] \geq 0.09/T$. 
$\Pr[|G| > 0.042n/T] \geq 0.65$ for sufficiently large $n$.
%\elaine{assumes T at least 2, T much smaller than n -- quantify how small}
\label{clm:Glarge}
\end{claim}
\begin{proof}
For a fixed $i \in [n]$, the probability that it lands in $R$
and is bad is  
\[
\frac{1}{10T} \cdot \left(1-\left(1-\frac{1}{10T}\right)^{T}\right)
\leq 
\frac{1}{10T} \cdot \left(1 - \frac{1}{(2e)^{0.1}}\right)
\leq 0.016/T
%\frac{1}{10T} \cdot (1-\frac{1}{10T})^T 
%\geq 0.09 /T
\]
\ignore{assumes T at least 2 above}

Therefore, the expected number of indices
that land in $R$ and are bad is at most 
$0.016 n/T$.
By Markov Inequality, 
with probability at most $1/3$, 
the number of indices
that land in $R$ and are bad exceeds  
$0.048 n/T$.

Due to \Cref{fct:Rsize}, 
and taking a union bound, 
with probability at least $1 - \exp(-\Omega(n/T)) - 1/3$, 
$|G| \geq 0.09n/T - 0.048n/T = 0.042n/T$.
\end{proof}

\begin{claim}
With probability at least $0.37$, %\elaine{FILL}, 
$|G_0| - |G_1| \geq 0.01\cdot n/T$.
\label{clm:g0minusg1}
\end{claim}
\begin{proof}
For a fixed $i \in [n]$, the probability that 
$i$ lands in $R$ and is good is 
\[
\frac{1}{10T} \cdot \left(1-\frac{1}{10T}\right)^T
\in [0.09/T, 0.091/T]
\]
Therefore, we have that
\[
\E[|G_0| - |G_1|]
\geq 
0.09/T \cdot \frac{3n}{4} 
- 
0.091/T \cdot \frac{n}{4} 
\geq 0.044 n/T
\]
Therefore,  
it must be that with probability $p \geq 0.37$, 
$|G_0| - |G_1| \geq 0.01\cdot n/T$, since otherwise, 
\[
\E[|G_0| - |G_1|]
\leq 
(p - \exp(-\Omega(n/T))) \cdot 0.09 n/T
 + (1-p) \cdot 0.01 n/T
+ \exp(-\Omega(n/T)) \cdot n
< 0.044 n/T
\]
\end{proof}

Combining \Cref{clm:Glarge}
and \Cref{clm:g0minusg1} and taking union
bound, we know that  
with probability at least
$1-(1-0.65)- (1-0.37) = 0.02$ over the choice of $R$, 
the following two good events hold: 
\begin{enumerate}[leftmargin=6mm]
\item  
$|G_0| - |G_1| \geq 0.01n/T$, and 
\item 
$|G| \geq 0.042n/T$.
\end{enumerate}

Below, we construct the encoding and decoding algorithm.
We sample some random coins that are shared
by the encoding and decoding algorithm, 
and these random coins are used to sample the set $R$.

\paragraph{Encoding.}
If the choice of $R$ is such that both of the above good events hold, then 
output the following encoding (otherwise output $\bot$):
\begin{enumerate}
\item 
The client's hint.
\item 
Values for the indices in $[n] \backslash R$.
\item 
Values for the indices in $R \backslash G$.
\item 
The set $G_1$.
\end{enumerate}
The client hint is at most $S$ bits.
The second part requires $n - |R|$ bits,
the third part requires $|R| - |G|$ bits, 
and the fourth part requires 
$|G| - c |G|$
bits for some constant $c \in (0, 1)$ 
by \Cref{fct:encodeset} and due to the 
aforementioned two good
events.  %$|G_0| - |G_1| \geq 0.01n/T$.
Therefore, the total encoding size
is at most 
$S + n - |R| + |R| - |G| + |G| - c|G| = S + n - c|G| \leq S + n - \Theta(n/T)$.

\paragraph{Decoding.}
\end{proof}



\section*{Old TEXT below}
\vspace{5mm}

For the proof, we will show that a solution to the PIR problem can be used to construct an algorithm to solve a probabilistic version of Yao's Box Problem.

\paragraph{Probabilistic Yao's Box Problem}
Suppose we have a working PIR scheme. Now, we will construct a solution to probabilistic Yao's Box Problem as follows:

\begin{itemize}
    \item Client's Hint: PIR's hint
    \item Query for $i \in [N]$: Run PIR for query $i$. If server looks at $\db[i]$, then output ``error".
\end{itemize}

% We stop at 46:00 in the recording.
We want to show the following. Given that PIRExpt: $i \xleftarrow{\$} [N]$, PIR preprocessing, PIR query on $i$,
\[p =\prob[\text{PIRExpt opens $i$}] \le \frac{T}{N} + \negl(N)\]

That is, we want to show that if we run a PIR query with the a random index $i$, the probability we open that index is small.

For a fixed $i$, define the following probability $p_i = \prob[\text{PIR on $i$ looks at $i$}].$ Then, 
\[p =\frac{1}{N} \sum_i p^i\]

Assume for the sake of contradiction that  $p > \frac{T}{N} + \mu$, where $\mu$ is non-negligible.

Let $p_{ji} = \prob[\text{PIR on $j$ opens $i$}]$. Since our scheme is private, the different indices should be computationally indistinguishable. Thus, this probability should be equally distributed. As a result,
\[p_{ji} = \prob[\text{PIR on $j$ opens $i$}] \ge p_i - \negl(N)\]

\begin{align*}
    E[\text{server work for PIR on $j$}] &\ge \sum_{i=1}^N p_{ji} \\
    &\ge \sum_{i=1}^N (p_{i} - \negl(N))\\
    &= Np - \negl(N) \tag{By def. of $p$}\\
    &> T + \mu N - \negl(N) \tag{As $p > \frac{T}{N} + \mu$}\\ 
\end{align*}

Thus, we have a contradiction because the expected number of locations the server needs to look at is strictly greater than $T$. Thus, we have shown that 

\[\prob[\text{PIRExpt opens $i$}] \le \frac{T}{N} + \negl(N)\]

\vspace{5mm}

Shifting our focus back to Yao's Box problem with probabilistic correctness on random index, the probabilistic correctness is
\[\prob[i \xleftarrow{\$} \text{correct for $i$}] \ge 1 - \frac{T}{N} - \negl(N)\]

\paragraph{Encoding Argument}
Randomness comes from two parts: the preprocessing part (the client's hint), and the query part. With this in mind, we will be using an augmented version of the encoding type argument in \ref{box_lower}.

Consider the following experiment.
\begin{enumerate}
    \item Run preprocessing, and choose a ``reasonably good hint." Initially, let the encoding be just the hint. We will add to this encoding as we go forward.
    \item Define an empty set called $\known = \{\}$
    \item In each step $i$, find the smallest $q_i \notin \known$. 
    \begin{itemize}
        \item If $\exists$ online coins such that query $q_i$ will give the correct answer, choose the lexicographically smallest coin. Execute query.
        \[\known \impliedby \cup \{q_i\} \cup \{\text{all newly opened}\}\]
        Add ``newly opened" to encoding.
        \item Else add $q_i$-th bit to the encoding.
    \end{itemize}
    \item Repeat until $\known = [N]$.
    
\end{enumerate}

A hint is bad for $i \in [N]$ if $\prob[\text{query $i$ correct} | \text{hint}] = 0$. That is, there does not exist an online coin such that the query is correct.

\begin{claim}
    $\exists$ hint that's bad for at most $T + 1$ location.
\end{claim}
\begin{proof}
    If all hints are bad for  more than $T+1$ locations, then
    \[\prob[i \in [N], \text{correct on $i$}] < 1 - \frac{T+1}{N}\]

    We have a contradiction, because it disagrees with our probabilistic correctness result above.
\end{proof}

Finally, we can reason about the encoding length. 

Suppose the worst case where the hint is bad for exactly $T+1$ locations. Let $\bbad$ be the encoding of the ``newly opened" boxes in the bad queries, and $\bgood$ be the encoding of the ``newly opened boxes in the good queries.
\begin{itemize}
    \item $|\bbad| = T + 1$, as each bad iteration adds one bit, and we have $T+1$ iterations.
    \item To find $|\bgood|$, we repeat the argument from \ref{box_lower}. Let $t_1, t_2, ...\ t_k$ be the number of newly opened boxes at each good step $i \in [k]$.

    By the ``plus one" argument, we have that $\sum_{i = 1}^k (t_i + 1)\ge N - (T + 1)$. We subtract $T + 1$ here because those indices have been handled by the bad iterations.
    
    Let $t = \frac{\sum_{i = 1}^k t_i}{k}$. Then as before, we have $k\ge \frac{N}{t+1}$
    \begin{align*}
        \sum_{i = 1}^k (t_i + 1)&\ge N - (T + 1) \\
        &\implies tk + k \ge N - T - 1 \\
        &\implies k \ge \frac{N - T - 1}{t+1}
    \end{align*}

    Thus, $|\bgood| = tk \ge t\frac{N - T - 1}{t+1}$
\end{itemize}

With the information above, we can mek the following conclusion.
\begin{align*}
    |\enc| &= S + |\bgood| + |\bbad| \ge N \\
    &\implies S + t\frac{N - T - 1}{t+1} + T + 1 \ge N \\
    &\implies S(t+1) + T + 1 \ge N \tag{By simplification}\\
    &\implies (S+1)(T+1) \ge N \tag{As $t$ upper bounded by $T$}
\end{align*}

\qed
