%\usepackage{xspace}

\newcommand{\bits}{\{0,1\}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfv}{\mathbf{v}}
\newcommand{\bfp}{\mathbf{p}}
\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfy}{\mathbf{y}}

\newcommand{\calR}{\mathcal{R}}

\newcommand{\CNextMsg}{\ensuremath{{\sf C Next Msg}}}
\newcommand{\SNextMsg}{\ensuremath{{\sf S Next Msg}}}
\newcommand{\CNext}{\ensuremath{{\sf C Next}}}
\newcommand{\SNext}{\ensuremath{{\sf S Next}}}
\newcommand{\Cstp}{\ensuremath{{\sf Cst'}}}
\newcommand{\Cst}{\ensuremath{{\sf Cst}}}
\newcommand{\msg}{\ensuremath{{\sf msg}}}
\newcommand{\msgp}{\ensuremath{{\sf msg'}}}
\newcommand{\Coutput}{\ensuremath{{\sf Reconstr}}}
\newcommand{\ans}{\ensuremath{{\sf ans}}}
\newcommand{\Ccoins}{\ensuremath{{\sf Ccoins}}}
\newcommand{\Scoins}{\ensuremath{{\sf Scoins}}}
%\newcommand{\Comm}{\ensuremath{{\sf Comm}}}
\newcommand{\Expt}{\ensuremath{{\sf Expt}}}
\newcommand{\coin}{\ensuremath{{\sf coin}}}
\newcommand{\View}{\ensuremath{{\sf View}}}
%\newcommand{\negl}{\ensuremath{{\sf negl}}}
\newcommand{\PPT}{PPT }
\newcommand{\Out}{\ensuremath{{\sf Out}}}
\newcommand{\OWF}{\ensuremath{{\sf OWF}}}
\newcommand{\OT}{\ensuremath{{\sf OT}}}
\newcommand{\PIR}{\ensuremath{{\sf PIR}}}
\newcommand{\Server}{\ensuremath{{\sf Server}}}
\newcommand{\Client}{\ensuremath{{\sf Client}}}
\newcommand{\Alice}{\ensuremath{{\sf Alice}}}
\newcommand{\Bob}{\ensuremath{{\sf Bob}}}
%\newcommand{\getr}{\ensuremath{~{\overset{\$}{\leftarrow}}}~}
\newcommand{\get}{\ensuremath{\leftarrow}}
\newcommand{\E}{\ensuremath{{\bf E}}}
\newcommand{\out}{\ensuremath{{\sf out}}}
%\newcommand{\PRF}{\ensuremath{{\sf PRF}}}
\newcommand{\prob}{\ensuremath{\mathbb{P}}}
\newcommand{\known}{\ensuremath{{\sf known}}}
\newcommand{\enc}{\ensuremath{{\sf enc}}}
\newcommand{\db}{\ensuremath{{\sf DB}}}
\newcommand{\bbad}{\ensuremath{{\sf b_{bad}}}}
\newcommand{\bgood}{\ensuremath{{\sf b_{good}}}}

\setlength{\parindent}{0pt}
%This is our final lecture on private information retrieval. 
In this lecture, we will prove a lower bound about the client space and server computation 
tradeoff for preprocessing PIR schemes. 
%For the proof, we will use techniques from complexity theory literature.
We will borrow techniques for proving time-space tradeoff
from the complexity theory literature.

Specifically, we consider a 1-server preprocessing PIR scheme 
in which 
the client and the server 
first performs some preprocessing over the database $\DB\in \{0, 1\}^n$.
The preprocessing can perform  
an unbounded amount 
of computation, at the end of which 
the client obtains an $S$-bit hint
at the end of the preprocessing, and the server stores only the original database
itself, and no extra information. 
Then, the client engages in a query protocol with the server,
to learn the database at some index  
$i \in [n]$.
To answer the query, the server is allowed to read at most $T$ locations of the database. 
For such a preprocessing PIR scheme, we will prove a tradeoff
between $S$ and $T$ as stated in the following theorem: 

\begin{theorem}[Time space tradeoff for preprocessing PIR~\cite{CK20}]
    Given a 1-server preprocessing PIR, 
        let $S$ be the client space, and let $T$ be the server compuation per query. 
    Then,  
    $(S+1)(T+1) \ge N$. \elaine{TODO: edit the theorem based on what we can prove later}
\label{thm:lb}
\end{theorem}

\paragraph{Piano.}
Recall that in an earlier lecture, 
we covered Piano~\cite{piano}, a preprocessing 1-server PIR scheme. 
It uses the client-specific preprocessing model, meaning that each client has a subscription phase with the server, during which it will perform preprocessing. Piano enjoys 
the following performance bounds:
\begin{itemize}[leftmargin=7mm]
    \item Client space: $\widetilde{O}(\sqrt{n})$
where $\widetilde{O}(\cdot)$ hides a(n  arbitrarily small) superlogarithmic function.
    \item Communication per query: $O(\sqrt{n})$
    \item Server computation per query: ${O}(\sqrt{n})$
\end{itemize}

We can see that Piano achieves optimal 
client space and server computation tradeoff (up to polylogarithmic factors)
in light of \Cref{thm:lb}.
%Consider the server computation and client space trade-off. For PIANO, this trade-off [Client Space: $\widetilde{O}(\sqrt{n})$, Compute per Query: $\widetilde{O}(\sqrt{n})$] is nearly optimal up to poly-log factors. 


%This is because of the following lower bound.
%\paragraph{Space/Computation Lower Bound}


%This lower bound holds even for a scheme that supports 1 query, regardless of server space, bandwidth, and amount of preprocessing work. 

Before proving \Cref{thm:lb}, 
we first prove a time-space tradeoff for a classical problem
called the Yao's box problem~\cite{yao}, and we shall
see why 
Yao's box problem is closely related to preprocessing PIR.

\section{Yao's Box Problem}
We have a server with $N$ boxes, each covering a bit. Note that this is not a PIR scheme, because it does not provide any privacy guarantees.

\paragraph{Preprocessing Phase}
Client and server can open all bits, and run arbitrary and unbounded computations. However, we have a constraint that at the end of the preprocessing, client can only store $S$ bits of information.

\paragraph{Query}
Client wants to know the i-th bit. We allow the client and server to have unbounded communication and work. We have a constraint that the server can open at most $T$ boxes and it cannot open box $i$.

\paragraph{Upper Bound}
\begin{enumerate}
    \item Divide the $N$ boxes into $\sqrt{N}$ segments. 
    \item Preprocessing Phase: Have the client store the parity of each segment.
    \item Query Phase ($i$): Server opens every box in $i$'s segment except $i$ and sends the parity back to the client. As the client knows the parity of each segment, it can easily reconstruct the value of bit $i$.
\end{enumerate}
Here, $S = \sqrt{N}$ and $T = \sqrt{N} - 1$

\paragraph{Lower Bound} \label{box_lower}
\begin{theorem}
    $S(T+1) \ge N$ 
\end{theorem}
\begin{proof}
    We will be using an encoding type argument. Consider the following experiment.
    \begin{enumerate}
        \item Run preprocessing
        \item Define an empty set called $\known = \{\}$. In each step $i$, the client finds the smallest $q_i \notin \known$, queries $q_i$.

        With this step, $\known \impliedby \known \cup \{q_i\} \cup {\text{all boxes opened during query}}$
        \item If $\known \neq [N]$, break. Otherwise, repeat step 2 again.
    \end{enumerate}

    Let the ``client's hint" denote whatever information that the client stores after the preprocessing phase. The hint is at most $S$ bits long.
    
    Define the encoding $\enc$ for this process as follows: 
    \[\enc = \text{client's hint} + \text{all ``newly opened" boxes in all queries}\]

    We write ``newly opened" because we do not want to include values that have already been recorded in the encoding.
    \vspace{5mm}
    
    By Shannon's theorem, we will show that $|\enc| \ge N$.

    Let $t_1, t_2, ...\ t_k$ be the number of newly opened boxes at each step $i \in [k]$. 
    
    Note here that we have the power of \textbf{plus one}; if I open $t$ boxes, I end up learning $t+1$ new bits. This is because I also learn the value of the query without opening its box.

    Thus, the $\known$ set increments with the following pattern.

    \begin{itemize}
        \item We newly open $t_1$ boxes: $\known$ increments by $t_1 + 1$
        \item We newly open $t_2$ boxes: $\known$ increments by $t_2 + 1$
        \item and so on...
    \end{itemize}

    Thus, we have that $\sum_{i = 1}^k (t_i + 1)\ge N$. Let $t = \frac{\sum_{i = 1}^k t_i}{k}$. Note that $t$ is the average number of boxes opened on each iteration so it must be upper bounded by $T$, the maximum number of boxes that can be opened in an iteration. Since we repeat the steps until $\known = [N]$, we have that
    \[\sum_{i = 1}^k (t_i + 1)\ge N \implies (t+1)k \ge N \implies k\ge \frac{N}{t+1} \tag{1}\]

    The encoding size is $S + \sum_{i=1}^k t_k = S + tk$. By Shannon's we have that $S + tk \ge N$. By (1), we have that 
    \begin{align*}
        S + tk &\ge N \\
        &\implies S + t \frac{N}{t+1} \ge N \tag{By (1)}\\
        &\implies S(t+1) \ge N \tag{Simplification}\\
        &\implies S(T+1) \ge N \tag{As $t \le T$}
    \end{align*}
\end{proof}

\section{PIR Lower Bound}
Using our knowledge of Yao's Box Problem, let's know try to prove the PIR lower bound.
\begin{theorem}
    Suppose we have a 1 server prepossessing PIR with perfect correctness and $negl(n)$ privacy loss with client space $S$ and server computation $T$ per query. Then,
    \[(S+1)(T+1) \ge  N \ \ \cite{CK20}\]
\end{theorem}
This lower bound holds even for computationally private schemes. It holds even for a single query, regardless of bandwidth, server space, even when preprocessing can be unbounded. However, we have a restriction that the server stores the original database and nothing else. That is, it does not store any encoding of the database.

\vspace{5mm}

For the proof, we will show that a solution to the PIR problem can be used to construct an algorithm to solve a probabilistic version of Yao's Box Problem.

\paragraph{Probabilistic Yao's Box Problem}
Suppose we have a working PIR scheme. Now, we will construct a solution to probabilistic Yao's Box Problem as follows:

\begin{itemize}
    \item Client's Hint: PIR's hint
    \item Query for $i \in [N]$: Run PIR for query $i$. If server looks at $\db[i]$, then output ``error".
\end{itemize}

% We stop at 46:00 in the recording.
We want to show the following. Given that PIRExpt: $i \xleftarrow{\$} [N]$, PIR preprocessing, PIR query on $i$,
\[p =\prob[\text{PIRExpt opens $i$}] \le \frac{T}{N} + \negl(N)\]

That is, we want to show that if we run a PIR query with the a random index $i$, the probability we open that index is small.

For a fixed $i$, define the following probability $p_i = \prob[\text{PIR on $i$ looks at $i$}].$ Then, 
\[p =\frac{1}{N} \sum_i p^i\]

Assume for the sake of contradiction that  $p > \frac{T}{N} + \mu$, where $\mu$ is non-negligible.

Let $p_{ji} = \prob[\text{PIR on $j$ opens $i$}]$. Since our scheme is private, the different indices should be computationally indistinguishable. Thus, this probability should be equally distributed. As a result,
\[p_{ji} = \prob[\text{PIR on $j$ opens $i$}] \ge p_i - \negl(N)\]

\begin{align*}
    E[\text{server work for PIR on $j$}] &\ge \sum_{i=1}^N p_{ji} \\
    &\ge \sum_{i=1}^N (p_{i} - \negl(N))\\
    &= Np - \negl(N) \tag{By def. of $p$}\\
    &> T + \mu N - \negl(N) \tag{As $p > \frac{T}{N} + \mu$}\\ 
\end{align*}

Thus, we have a contradiction because the expected number of locations the server needs to look at is strictly greater than $T$. Thus, we have shown that 

\[\prob[\text{PIRExpt opens $i$}] \le \frac{T}{N} + \negl(N)\]

\vspace{5mm}

Shifting our focus back to Yao's Box problem with probabilistic correctness on random index, the probabilistic correctness is
\[\prob[i \xleftarrow{\$} \text{correct for $i$}] \ge 1 - \frac{T}{N} - \negl(N)\]

\paragraph{Encoding Argument}
Randomness comes from two parts: the preprocessing part (the client's hint), and the query part. With this in mind, we will be using an augmented version of the encoding type argument in \ref{box_lower}.

Consider the following experiment.
\begin{enumerate}
    \item Run preprocessing, and choose a ``reasonably good hint." Initially, let the encoding be just the hint. We will add to this encoding as we go forward.
    \item Define an empty set called $\known = \{\}$
    \item In each step $i$, find the smallest $q_i \notin \known$. 
    \begin{itemize}
        \item If $\exists$ online coins such that query $q_i$ will give the correct answer, choose the lexicographically smallest coin. Execute query.
        \[\known \impliedby \cup \{q_i\} \cup \{\text{all newly opened}\}\]
        Add ``newly opened" to encoding.
        \item Else add $q_i$-th bit to the encoding.
    \end{itemize}
    \item Repeat until $\known = [N]$.
    
\end{enumerate}

A hint is bad for $i \in [N]$ if $\prob[\text{query $i$ correct} | \text{hint}] = 0$. That is, there does not exist an online coin such that the query is correct.

\begin{claim}
    $\exists$ hint that's bad for at most $T + 1$ location.
\end{claim}
\begin{proof}
    If all hints are bad for  more than $T+1$ locations, then
    \[\prob[i \in [N], \text{correct on $i$}] < 1 - \frac{T+1}{N}\]

    We have a contradiction, because it disagrees with our probabilistic correctness result above.
\end{proof}

Finally, we can reason about the encoding length. 

Suppose the worst case where the hint is bad for exactly $T+1$ locations. Let $\bbad$ be the encoding of the ``newly opened" boxes in the bad queries, and $\bgood$ be the encoding of the ``newly opened boxes in the good queries.
\begin{itemize}
    \item $|\bbad| = T + 1$, as each bad iteration adds one bit, and we have $T+1$ iterations.
    \item To find $|\bgood|$, we repeat the argument from \ref{box_lower}. Let $t_1, t_2, ...\ t_k$ be the number of newly opened boxes at each good step $i \in [k]$.

    By the ``plus one" argument, we have that $\sum_{i = 1}^k (t_i + 1)\ge N - (T + 1)$. We subtract $T + 1$ here because those indices have been handled by the bad iterations.
    
    Let $t = \frac{\sum_{i = 1}^k t_i}{k}$. Then as before, we have $k\ge \frac{N}{t+1}$
    \begin{align*}
        \sum_{i = 1}^k (t_i + 1)&\ge N - (T + 1) \\
        &\implies tk + k \ge N - T - 1 \\
        &\implies k \ge \frac{N - T - 1}{t+1}
    \end{align*}

    Thus, $|\bgood| = tk \ge t\frac{N - T - 1}{t+1}$
\end{itemize}

With the information above, we can mek the following conclusion.
\begin{align*}
    |\enc| &= S + |\bgood| + |\bbad| \ge N \\
    &\implies S + t\frac{N - T - 1}{t+1} + T + 1 \ge N \\
    &\implies S(t+1) + T + 1 \ge N \tag{By simplification}\\
    &\implies (S+1)(T+1) \ge N \tag{As $t$ upper bounded by $T$}
\end{align*}

\qed
